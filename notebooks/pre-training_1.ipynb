{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credit to https://www.kaggle.com/wfwiggins203 in https://www.kaggle.com/wfwiggins203/eda-dicom-tags-windowing-head-cts\n",
    "\n",
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pydicom\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Sigmoid\n",
    "from torchvision.transforms import functional as F\n",
    "from torchvision.transforms import Normalize\n",
    "from math import log\n",
    "from numpy import e\n",
    "%matplotlib inline\n",
    "plt.style.use('grayscale')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.cuda.current_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = \"C:/Users/evbruh/Downloads/rsna-intracranial-hemorrhage-detection/\"\n",
    "TRAIN_DIR = ROOT_DIR + 'stage_1_train_images'\n",
    "TEST_DIR = ROOT_DIR + 'stage_1_test_images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.read_csv(ROOT_DIR + 'stage_1_train.csv')\n",
    "print(all_df.shape)\n",
    "all_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df[['ID', 'Subtype']] = all_df['ID'].str.rsplit(pat='_', n=1, expand=True)\n",
    "print(all_df.shape)\n",
    "all_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_id(img_id, img_dir=TRAIN_DIR):\n",
    "    if not re.match(r'ID_[a-z0-9]+', img_id):\n",
    "        sop = re.search(r'[a-z0-9]+', img_id)\n",
    "        if sop:\n",
    "            img_id_new = f'ID_{sop[0]}'\n",
    "            return img_id_new\n",
    "        else:\n",
    "            print(img_id)\n",
    "    return img_id\n",
    "\n",
    "def id_to_filepath(img_id, img_dir=TRAIN_DIR):\n",
    "    filepath = f'{img_dir}/{img_id}.dcm' # pydicom doesn't play nice with Path objects\n",
    "    if os.path.exists(filepath):\n",
    "        return filepath\n",
    "    else:\n",
    "        return 'DNE'\n",
    "    \n",
    "def get_patient_data(filepath):\n",
    "    if filepath != 'DNE':\n",
    "        dcm_data = pydicom.dcmread(filepath, stop_before_pixels=True)\n",
    "        return dcm_data.PatientID, dcm_data.StudyInstanceUID, dcm_data.SeriesInstanceUID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df['ID'] = all_df['ID'].apply(fix_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this method also handles duplicates gracefully\n",
    "all_df = all_df.pivot_table(index='ID', columns='Subtype').reset_index()\n",
    "print(all_df.shape)\n",
    "all_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_df['filepath'] = all_df['ID'].apply(id_to_filepath)\n",
    "all_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = all_df[:int(.9*all_df.shape[0])]\n",
    "val_df = all_df[int(.9*all_df.shape[0]):]\n",
    "all_df.shape, train_df.shape, val_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hem_types = ['epidural', 'intraparenchymal', 'intraventricular', 'subarachnoid', 'subdural']\n",
    "\n",
    "def load_random_images():\n",
    "    image_names = [list(train_df[train_df['Label', h_type] == 1].sample(1)['filepath'])[0] for h_type in hem_types]\n",
    "    image_names += list(train_df[train_df['Label', 'any'] == 0].sample(5)['filepath'])\n",
    "    return [pydicom.read_file(os.path.join(ROOT_DIR, img_name)) for img_name in image_names]\n",
    "\n",
    "def view_images(images):\n",
    "    width = 5\n",
    "    height = 2\n",
    "    fig, axs = plt.subplots(height, width, figsize=(15,5))\n",
    "    \n",
    "    for im in range(0, height * width):\n",
    "        image = images[im]\n",
    "        i = im // width\n",
    "        j = im % width\n",
    "        axs[i,j].imshow(image, cmap=plt.cm.bone) \n",
    "        axs[i,j].axis('off')\n",
    "        title = hem_types[im] if im < len(hem_types) else 'normal'\n",
    "        axs[i,j].set_title(title)\n",
    "\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = load_random_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "from random import random as rnd, shuffle\n",
    "from torchvision import transforms\n",
    "from scipy.stats import truncnorm, uniform\n",
    "from torchvision.transforms.functional import adjust_brightness, adjust_contrast, resized_crop, rotate\n",
    "\n",
    "eps = (1.0 / 255.0)\n",
    "ue = log((1.0 / eps) - 1.0)\n",
    "\n",
    "\n",
    "class sigmoid_windowing(nn.Module):\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "    \n",
    "        super(sigmoid_windowing, self).__init__()\n",
    "    \n",
    "        self.eps = eps\n",
    "        self.ue = ue\n",
    "        self.e = e\n",
    "    \n",
    "        self.w0 = (2 / 80) * ue\n",
    "        self.b0 = ((-2 * 40) / 80) * ue\n",
    "    \n",
    "        self.w1 = (2 / 200) * ue\n",
    "        self.b1 = ((-2 * 80) / 200) * ue\n",
    "    \n",
    "        self.w2 = (2 / 2000) * ue\n",
    "        self.b2 = ((-2 * 600) / 2000) * ue\n",
    "        \n",
    "        self.vdim = (512, 512, -1)\n",
    "        \n",
    "        self.n = Normalize(mean = [0, 0, 0],\n",
    "                           std = [1, 1, 1])\n",
    "    \n",
    "    def forward(self, dcm):\n",
    "\n",
    "        y = torch.Tensor(dcm.pixel_array.astype('float32')).to(device) * dcm.RescaleSlope + dcm.RescaleIntercept\n",
    "        \n",
    "        y0 = (1.0 / (1 + self.e**(-1.0 * (self.w0 * y + self.b0)))).view(self.vdim)\n",
    "        y1 = (1.0 / (1 + self.e**(-1.0 * (self.w1 * y + self.b1)))).view(self.vdim)\n",
    "        y2 = (1.0 / (1 + self.e**(-1.0 * (self.w2 * y + self.b2)))).view(self.vdim)\n",
    "\n",
    "        return self.n(torch.cat((y0, y1, y2), 2))\n",
    "    \n",
    "    \n",
    "class DICOMPreprocessor():\n",
    "    \n",
    "    def __init__(self, augment = True):\n",
    "        \n",
    "        self.windower = sigmoid_windowing()\n",
    "        \n",
    "        self.augment = augment\n",
    "        \n",
    "        self.normalize = lambda x: F.normalize(x, (0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "        \n",
    "        if augment:\n",
    "        \n",
    "            random_resized_crop = lambda x, d: F.resized_crop(x, x.size[0]//2-d, x.size[1]//2-d, d*2, d*2, size=(x.size[0], x.size[1]))\n",
    "        \n",
    "            brightness_params = truncnorm(1-.08, 1+.08), .5\n",
    "            contrast_params = truncnorm(1-.08, 1+.08), .5\n",
    "            random_resized_params = uniform(.7*512//2, .3*512//2), 0\n",
    "            rotate_params = uniform(-30, 60), .3\n",
    "            \n",
    "            self.FT = [(adjust_brightness, brightness_params),\n",
    "                   (adjust_contrast, contrast_params),\n",
    "                   (random_resized_crop, random_resized_params),\n",
    "                   (rotate, rotate_params)\n",
    "                  ]\n",
    "        \n",
    "    def __call__(self, img_name, output_tensor = True):\n",
    "        dcm = pydicom.read_file(img_name[0])     \n",
    "        x = self.windower(dcm)\n",
    "        if self.augment:\n",
    "            x = F.to_pil_image((255*x).cpu().numpy().astype('uint8'))\n",
    "            if rnd() < .5:\n",
    "                x = F.hflip(x)\n",
    "            if rnd() < .5:\n",
    "                x = F.vflip(x)\n",
    "            shuffle(self.FT)\n",
    "            for ft, (d, p) in self.FT:\n",
    "                if rnd() < p:\n",
    "                    x = ft(x, d.rvs())\n",
    "        if output_tensor:\n",
    "            return self.normalize(F.to_tensor(x)).unsqueeze(0)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "train_dicom_preprocessor = DICOMPreprocessor(augment=True)\n",
    "val_dicom_preprocessor = DICOMPreprocessor(augment=True)\n",
    "test_dicom_preprocessor = DICOMPreprocessor(augment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "models.densenet121(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = models.densenet121(pretrained=True)\n",
    "features = nn.Sequential(*list(model_ft.features)[:-1])\n",
    "classifier = nn.Sequential(\n",
    "    nn.Upsample(size=(16, 16), mode='bilinear'),\n",
    "    nn.BatchNorm2d(1024),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(1024, 6, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
    "    nn.BatchNorm2d(1024),\n",
    "    nn.Tanh()\n",
    ")\n",
    "model_ft.classifier = classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_to_update = [\n",
    "    'features.denseblock4.denselayer1.norm1.weight',\n",
    "    'features.denseblock4.denselayer1.norm1.bias',\n",
    "    'features.denseblock4.denselayer1.conv1.weight',\n",
    "    'features.denseblock4.denselayer1.norm2.weight',\n",
    "    'features.denseblock4.denselayer1.norm2.bias',\n",
    "    'features.denseblock4.denselayer1.conv2.weight',\n",
    "    'features.denseblock4.denselayer2.norm1.weight',\n",
    "    'features.denseblock4.denselayer2.norm1.bias',\n",
    "    'features.denseblock4.denselayer2.conv1.weight',\n",
    "    'features.denseblock4.denselayer2.norm2.weight',\n",
    "    'features.denseblock4.denselayer2.norm2.bias',\n",
    "    'features.denseblock4.denselayer2.conv2.weight',\n",
    "    'features.denseblock4.denselayer3.norm1.weight',\n",
    "    'features.denseblock4.denselayer3.norm1.bias',\n",
    "    'features.denseblock4.denselayer3.conv1.weight',\n",
    "    'features.denseblock4.denselayer3.norm2.weight',\n",
    "    'features.denseblock4.denselayer3.norm2.bias',\n",
    "    'features.denseblock4.denselayer3.conv2.weight',\n",
    "    'features.denseblock4.denselayer4.norm1.weight',\n",
    "    'features.denseblock4.denselayer4.norm1.bias',\n",
    "    'features.denseblock4.denselayer4.conv1.weight',\n",
    "    'features.denseblock4.denselayer4.norm2.weight',\n",
    "    'features.denseblock4.denselayer4.norm2.bias',\n",
    "    'features.denseblock4.denselayer4.conv2.weight',\n",
    "    'features.denseblock4.denselayer5.norm1.weight',\n",
    "    'features.denseblock4.denselayer5.norm1.bias',\n",
    "    'features.denseblock4.denselayer5.conv1.weight',\n",
    "    'features.denseblock4.denselayer5.norm2.weight',\n",
    "    'features.denseblock4.denselayer5.norm2.bias',\n",
    "    'features.denseblock4.denselayer5.conv2.weight',\n",
    "    'features.denseblock4.denselayer6.norm1.weight',\n",
    "    'features.denseblock4.denselayer6.norm1.bias',\n",
    "    'features.denseblock4.denselayer6.conv1.weight',\n",
    "    'features.denseblock4.denselayer6.norm2.weight',\n",
    "    'features.denseblock4.denselayer6.norm2.bias',\n",
    "    'features.denseblock4.denselayer6.conv2.weight',\n",
    "    'features.denseblock4.denselayer7.norm1.weight',\n",
    "    'features.denseblock4.denselayer7.norm1.bias',\n",
    "    'features.denseblock4.denselayer7.conv1.weight',\n",
    "    'features.denseblock4.denselayer7.norm2.weight',\n",
    "    'features.denseblock4.denselayer7.norm2.bias',\n",
    "    'features.denseblock4.denselayer7.conv2.weight',\n",
    "    'features.denseblock4.denselayer8.norm1.weight',\n",
    "    'features.denseblock4.denselayer8.norm1.bias',\n",
    "    'features.denseblock4.denselayer8.conv1.weight',\n",
    "    'features.denseblock4.denselayer8.norm2.weight',\n",
    "    'features.denseblock4.denselayer8.norm2.bias',\n",
    "    'features.denseblock4.denselayer8.conv2.weight',\n",
    "    'features.denseblock4.denselayer9.norm1.weight',\n",
    "    'features.denseblock4.denselayer9.norm1.bias',\n",
    "    'features.denseblock4.denselayer9.conv1.weight',\n",
    "    'features.denseblock4.denselayer9.norm2.weight',\n",
    "    'features.denseblock4.denselayer9.norm2.bias',\n",
    "    'features.denseblock4.denselayer9.conv2.weight',\n",
    "    'features.denseblock4.denselayer10.norm1.weight',\n",
    "    'features.denseblock4.denselayer10.norm1.bias',\n",
    "    'features.denseblock4.denselayer10.conv1.weight',\n",
    "    'features.denseblock4.denselayer10.norm2.weight',\n",
    "    'features.denseblock4.denselayer10.norm2.bias',\n",
    "    'features.denseblock4.denselayer10.conv2.weight',\n",
    "    'features.denseblock4.denselayer11.norm1.weight',\n",
    "    'features.denseblock4.denselayer11.norm1.bias',\n",
    "    'features.denseblock4.denselayer11.conv1.weight',\n",
    "    'features.denseblock4.denselayer11.norm2.weight',\n",
    "    'features.denseblock4.denselayer11.norm2.bias',\n",
    "    'features.denseblock4.denselayer11.conv2.weight',\n",
    "    'features.denseblock4.denselayer12.norm1.weight',\n",
    "    'features.denseblock4.denselayer12.norm1.bias',\n",
    "    'features.denseblock4.denselayer12.conv1.weight',\n",
    "    'features.denseblock4.denselayer12.norm2.weight',\n",
    "    'features.denseblock4.denselayer12.norm2.bias',\n",
    "    'features.denseblock4.denselayer12.conv2.weight',\n",
    "    'features.denseblock4.denselayer13.norm1.weight',\n",
    "    'features.denseblock4.denselayer13.norm1.bias',\n",
    "    'features.denseblock4.denselayer13.conv1.weight',\n",
    "    'features.denseblock4.denselayer13.norm2.weight',\n",
    "    'features.denseblock4.denselayer13.norm2.bias',\n",
    "    'features.denseblock4.denselayer13.conv2.weight',\n",
    "    'features.denseblock4.denselayer14.norm1.weight',\n",
    "    'features.denseblock4.denselayer14.norm1.bias',\n",
    "    'features.denseblock4.denselayer14.conv1.weight',\n",
    "    'features.denseblock4.denselayer14.norm2.weight',\n",
    "    'features.denseblock4.denselayer14.norm2.bias',\n",
    "    'features.denseblock4.denselayer14.conv2.weight',\n",
    "    'features.denseblock4.denselayer15.norm1.weight',\n",
    "    'features.denseblock4.denselayer15.norm1.bias',\n",
    "    'features.denseblock4.denselayer15.conv1.weight',\n",
    "    'features.denseblock4.denselayer15.norm2.weight',\n",
    "    'features.denseblock4.denselayer15.norm2.bias',\n",
    "    'features.denseblock4.denselayer15.conv2.weight',\n",
    "    'features.denseblock4.denselayer16.norm1.weight',\n",
    "    'features.denseblock4.denselayer16.norm1.bias',\n",
    "    'features.denseblock4.denselayer16.conv1.weight',\n",
    "    'features.denseblock4.denselayer16.norm2.weight',\n",
    "    'features.denseblock4.denselayer16.norm2.bias',\n",
    "    'features.denseblock4.denselayer16.conv2.weight',\n",
    "    'features.norm5.weight',\n",
    "    'features.norm5.bias',\n",
    "    'classifier.1.weight',\n",
    "    'classifier.1.bias',\n",
    "    'classifier.3.weight',\n",
    "    'classifier.4.weight',\n",
    "    'classifier.4.bias'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model_ft.named_parameters():\n",
    "    if name in params_to_update:\n",
    "        param.requires_grad = True\n",
    "        print(\"\\t\",name)\n",
    "    else:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import torch.nn as nn\n",
    "# from torchvision import models\n",
    "# import torch.nn.functional as nnF\n",
    "\n",
    "# #\n",
    "# def get_dcnn_base(arch, pretrained):\n",
    "#     if arch == 'densenet121':\n",
    "#         model = models.densenet121(pretrained=pretrained)\n",
    "#         model = nn.Sequential(*list(model.features)[:-1])\n",
    "#     else:\n",
    "#         print('No {}!'.format(arch))\n",
    "#         raise ValueError\n",
    "#     return model\n",
    "\n",
    "# #\n",
    "# class DenseNet121(nn.Module):\n",
    "\n",
    "#     def __init__(self, num_classes=6, out_hw=16, pretrained=True):\n",
    "#         super(DenseNet121, self).__init__()\n",
    "#         self.features = get_dcnn_base('densenet121', pretrained)\n",
    "#         self.up = nn.Upsample(size=(out_hw, out_hw), mode='bilinear')\n",
    "#         self.bn1 = nn.BatchNorm2d(1024)\n",
    "#         self.conv1 = nn.Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "#         self.bn2 = nn.BatchNorm2d(512)\n",
    "#         self.conv2 = nn.Conv2d(512, num_classes, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "        \n",
    "#     def forward(self, x) -> 'PxPxK tensor prediction':\n",
    "#         x = self.features(x)\n",
    "#         x = self.up(x)\n",
    "#         x = self.bn1(x)\n",
    "#         x = nnF.relu(x)\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.bn2(x)\n",
    "#         x = nnF.relu(x)\n",
    "#         x = self.conv2(x)\n",
    "#         x = nnF.sigmoid(x)\n",
    "#         return x\n",
    "#\n",
    "# model = DenseNet121()\n",
    "# model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "optimizer_ft = optim.SGD(filter(lambda p: p.requires_grad, model_ft.parameters()), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myCrossEntropyLoss(outputs, targets, weight):\n",
    "    outputs = 1 - (1-outputs).reshape(-1,6,256).prod(2)\n",
    "    bceloss = nn.BCELoss(weight)\n",
    "    return bceloss(outputs, targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import threading\n",
    "from queue import Queue\n",
    "from random import random as rnd, shuffle\n",
    "from torchvision import transforms\n",
    "from scipy.stats import truncnorm, uniform\n",
    "from torchvision.transforms.functional import adjust_brightness, adjust_contrast, resized_crop, rotate\n",
    "\n",
    "\n",
    "eps = (1.0 / 255.0)\n",
    "ue = log((1.0 / eps) - 1.0)\n",
    "\n",
    "batch_size = 32\n",
    "batch_dim = (batch_size, 3, 512, 512)\n",
    "batchX = torch.zeros(batch_dim).to(device)\n",
    "batchY = torch.zeros((batch_size, 6)).to(device)\n",
    "\n",
    "\n",
    "class StoppableThread(threading.Thread):\n",
    "    \"\"\"Thread class with a stop() method. The thread itself has to check\n",
    "    regularly for the stopped() condition.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(StoppableThread, self).__init__()\n",
    "        self._stop_event = threading.Event()\n",
    "\n",
    "    def stop(self):\n",
    "        self._stop_event.set()\n",
    "\n",
    "    def stopped(self):\n",
    "        return self._stop_event.is_set()\n",
    "\n",
    "    \n",
    "class Preprocessor(threading.Thread):\n",
    "\n",
    "    def __init__(self, preprocessor = None, labels = True, idx = None, row = None):\n",
    "        super(Preprocessor, self).__init__()\n",
    "        self.preprocessor = preprocessor\n",
    "        self.labels = labels\n",
    "        self.idx = idx\n",
    "        self.row = row\n",
    "        self.display = False\n",
    "        self.device = torch.cuda.current_device()\n",
    "        \n",
    "    def run(self):\n",
    "        \n",
    "        global batchX\n",
    "        \n",
    "        img_name = self.row.filepath\n",
    "        \n",
    "        img = self.preprocessor.__call__(img_name, output_tensor = not self.display)\n",
    "            \n",
    "        if self.labels:\n",
    "            global batchY\n",
    "            \n",
    "            labels = torch.tensor(self.row.Label)\n",
    "            batchX[self.idx], batchY[self.idx] = img.to(self.device), labels.to(self.device)\n",
    "            del img, labels\n",
    "        \n",
    "        else:      \n",
    "            \n",
    "            batchX[self.idx] = img\n",
    "            del img\n",
    "    \n",
    "\n",
    "class Controller():\n",
    "\n",
    "    def __init__(self, data, labels = True, batch_size = 32):\n",
    "        \n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.N = self.__len__()\n",
    "        self.t_ct = 0 # total count\n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.data)\n",
    "    \n",
    "    def __shuffle__(): # TODO\n",
    "        pass\n",
    "    \n",
    "    def __epoch__():\n",
    "        self.t_ct = 0\n",
    "    \n",
    "    def __itrn__(self):\n",
    "        \n",
    "        idx = int(self.t_ct%self.batch_size)\n",
    "        \n",
    "        row = self.data.loc[self.t_ct]\n",
    "        \n",
    "        self.t_ct += 1\n",
    "        \n",
    "        return idx, row\n",
    "\n",
    "    \n",
    "class Worker(threading.Thread):\n",
    "    \n",
    "    def __init__(self, q):\n",
    "        self.q = q\n",
    "        super(Worker, self).__init__()\n",
    "        \n",
    "    def run(self, process):\n",
    "        self.q.put(process)            \n",
    "    \n",
    "    \n",
    "class DataLoader:\n",
    "        \n",
    "    def __init__(self, controller, preprocessor, batch_size = 32, shuffle = True, num_workers = 8):\n",
    "        self.controller = controller\n",
    "        self.preprocessor = preprocessor\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.num_workers = num_workers\n",
    "        self.workers = {i:Worker(Queue()) for i in range(num_workers)}\n",
    "        \n",
    "    def batch(self):\n",
    "        for i in range(self.batch_size):\n",
    "            worker = int(i%self.num_workers)\n",
    "            row_idx, row_data = self.controller.__itrn__()\n",
    "            processor = Preprocessor(self.preprocessor, idx = row_idx, row = row_data)\n",
    "            self.workers[worker].q.put(processor.start())\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_normalize = transforms.Normalize(\n",
    "    mean=[-0.485/0.229, -0.456/0.224, -0.406/0.255],\n",
    "    std=[1/0.229, 1/0.224, 1/0.255]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_controller = Controller(data=train_df)\n",
    "val_controller = Controller(data=val_df)\n",
    "\n",
    "train_dataloader = DataLoader(train_controller, train_dicom_preprocessor)\n",
    "val_dataloader = DataLoader(val_controller, val_dicom_preprocessor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.memory_cached()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader.workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.autograd import Variable\n",
    " \n",
    "for __ in  tqdm(range(5)):\n",
    "    train_dataloader.batch()\n",
    "    optimizer_ft.zero_grad()\n",
    "\n",
    "    model.train()\n",
    "    outputs = model(Variable(batchX))\n",
    "    loss = myCrossEntropyLoss(outputs, Variable(batchY), weight=torch.Tensor([2,1,1,1,1,1]).to(device))\n",
    "    print(loss)\n",
    "    loss.backward()\n",
    "    optimizer_ft.step()\n",
    "    del outputs, loss, inputs, labels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader.controller.t_ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(inv_normalize(batchX[-1]).cpu().permute (1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
